{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS324: Deep Learning - Assignment 3\n",
    "\n",
    "This notebook contains instructions for running the code and demonstrating the results for Assignment 3.\n",
    "\n",
    "## Contents\n",
    "- Part I: PyTorch LSTM for Palindrome Prediction\n",
    "- Part II: Generative Adversarial Networks (GAN) for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part I: PyTorch LSTM (40 points)\n",
    "\n",
    "## Task 1 & 2: LSTM Implementation and Training\n",
    "\n",
    "The LSTM is implemented without using `torch.nn.LSTM`, following the equations provided in the assignment:\n",
    "\n",
    "$$g^{(t)} = \\tanh(W_{gx}x^{(t)} + W_{gh}h^{(t-1)} + b_g)$$\n",
    "$$i^{(t)} = \\sigma(W_{ix}x^{(t)} + W_{ih}h^{(t-1)} + b_i)$$\n",
    "$$f^{(t)} = \\sigma(W_{fx}x^{(t)} + W_{fh}h^{(t-1)} + b_f)$$\n",
    "$$o^{(t)} = \\sigma(W_{ox}x^{(t)} + W_{oh}h^{(t-1)} + b_o)$$\n",
    "$$c^{(t)} = g^{(t)} \\odot i^{(t)} + c^{(t-1)} \\odot f^{(t)}$$\n",
    "$$h^{(t)} = \\tanh(c^{(t)}) \\odot o^{(t)}$$\n",
    "$$p^{(t)} = W_{ph}h^{(t)} + b_p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Part 1 to path\n",
    "sys.path.insert(0, 'Part 1')\n",
    "from lstm import LSTM\n",
    "from dataset import PalindromeDataset\n",
    "from utils import AverageMeter, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM on Palindrome Task\n",
    "\n",
    "Run the training script from command line:\n",
    "\n",
    "```bash\n",
    "cd \"Part 1\"\n",
    "python train.py --input_length 4 --max_epoch 50 --learning_rate 0.001\n",
    "```\n",
    "\n",
    "Or run the training in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Configuration\n",
    "input_length = 4  # For T=5 palindrome (input_length + 1)\n",
    "input_dim = 1\n",
    "num_hidden = 128\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "max_epoch = 20\n",
    "data_size = 10000\n",
    "portion_train = 0.8\n",
    "\n",
    "# Initialize model\n",
    "model = LSTM(\n",
    "    seq_length=input_length,\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=num_hidden,\n",
    "    output_dim=num_classes\n",
    ").to(device)\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = PalindromeDataset(input_length=input_length, total_len=data_size)\n",
    "train_size = int(portion_train * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Setup optimizer and loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Training LSTM on palindrome task with T={input_length+1}\")\n",
    "print(f\"Training samples: {train_size}, Validation samples: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # Train\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += accuracy(outputs, batch_targets)\n",
    "    \n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    train_accs.append(epoch_acc / len(train_loader))\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_targets in val_loader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy(outputs, batch_targets)\n",
    "    \n",
    "    val_losses.append(epoch_loss / len(val_loader))\n",
    "    val_accs.append(epoch_acc / len(val_loader))\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{max_epoch}: Train Loss: {train_losses[-1]:.4f}, \"\n",
    "              f\"Train Acc: {train_accs[-1]:.2f}%, Val Acc: {val_accs[-1]:.2f}%\")\n",
    "\n",
    "print(f\"\\nFinal Validation Accuracy: {val_accs[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(train_losses, label='Train')\n",
    "ax1.plot(val_losses, label='Validation')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_accs, label='Train')\n",
    "ax2.plot(val_accs, label='Validation')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_training_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part II: Generative Adversarial Networks (60 points)\n",
    "\n",
    "## Task 1: GAN Implementation\n",
    "\n",
    "The GAN is trained on MNIST with the following architecture:\n",
    "\n",
    "**Generator:**\n",
    "- Linear(latent_dim → 128) → LeakyReLU(0.2)\n",
    "- Linear(128 → 256) → BatchNorm → LeakyReLU(0.2)\n",
    "- Linear(256 → 512) → BatchNorm → LeakyReLU(0.2)\n",
    "- Linear(512 → 1024) → BatchNorm → LeakyReLU(0.2)\n",
    "- Linear(1024 → 784) → Tanh\n",
    "\n",
    "**Discriminator:**\n",
    "- Linear(784 → 512) → LeakyReLU(0.2)\n",
    "- Linear(512 → 256) → LeakyReLU(0.2)\n",
    "- Linear(256 → 1) → Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Part 2 to path\n",
    "sys.path.insert(0, 'Part 2')\n",
    "from my_gan import Generator, Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GAN on MNIST\n",
    "\n",
    "Run the training script from command line:\n",
    "\n",
    "```bash\n",
    "cd \"Part 2\"\n",
    "python my_gan.py --n_epochs 200 --batch_size 64 --lr 0.0002\n",
    "```\n",
    "\n",
    "Or train in the notebook (with fewer epochs for demonstration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# GAN Configuration\n",
    "latent_dim = 100\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "n_epochs = 50  # Reduced for notebook demo\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('Part 2/images', exist_ok=True)\n",
    "\n",
    "# Load MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/mnist', train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "print(f\"Training GAN on MNIST for {n_epochs} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store images at different training stages for Task 2\n",
    "images_start = None\n",
    "images_mid = None\n",
    "images_end = None\n",
    "\n",
    "# Fixed noise for consistent visualization\n",
    "fixed_noise = torch.randn(25, latent_dim, device=device)\n",
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_g_loss = 0.0\n",
    "    epoch_d_loss = 0.0\n",
    "    \n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        batch_size_curr = imgs.size(0)\n",
    "        real_imgs = imgs.to(device)\n",
    "        \n",
    "        # Labels\n",
    "        real_labels = torch.ones(batch_size_curr, 1, device=device)\n",
    "        fake_labels = torch.zeros(batch_size_curr, 1, device=device)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_output = discriminator(real_imgs)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "        \n",
    "        z = torch.randn(batch_size_curr, latent_dim, device=device)\n",
    "        fake_imgs = generator(z)\n",
    "        fake_output = discriminator(fake_imgs.detach())\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        z = torch.randn(batch_size_curr, latent_dim, device=device)\n",
    "        gen_imgs = generator(z)\n",
    "        output = discriminator(gen_imgs)\n",
    "        g_loss = criterion(output, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        epoch_g_loss += g_loss.item()\n",
    "        epoch_d_loss += d_loss.item()\n",
    "    \n",
    "    g_losses.append(epoch_g_loss / len(dataloader))\n",
    "    d_losses.append(epoch_d_loss / len(dataloader))\n",
    "    \n",
    "    # Save images at start, middle, and end\n",
    "    if epoch == 0:\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            images_start = generator(fixed_noise).cpu()\n",
    "        generator.train()\n",
    "    elif epoch == n_epochs // 2:\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            images_mid = generator(fixed_noise).cpu()\n",
    "        generator.train()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: D Loss: {d_losses[-1]:.4f}, G Loss: {g_losses[-1]:.4f}\")\n",
    "\n",
    "# Final images\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    images_end = generator(fixed_noise).cpu()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Sample 25 Images at Different Training Stages\n",
    "\n",
    "Display generated images at the start, middle, and end of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, title):\n",
    "    \"\"\"Display a grid of images\"\"\"\n",
    "    images = (images + 1) / 2  # Denormalize from [-1, 1] to [0, 1]\n",
    "    grid = make_grid(images, nrow=5, padding=2)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'{title.replace(\" \", \"_\").lower()}.png')\n",
    "    plt.show()\n",
    "\n",
    "if images_start is not None:\n",
    "    show_images(images_start, 'Start of Training (Epoch 1)')\n",
    "if images_mid is not None:\n",
    "    show_images(images_mid, f'Halfway Through Training (Epoch {n_epochs//2})')\n",
    "if images_end is not None:\n",
    "    show_images(images_end, f'End of Training (Epoch {n_epochs})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Latent Space Interpolation\n",
    "\n",
    "Interpolate between two different generated digits in latent space with 7 interpolation steps (9 images total including endpoints):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_latent(generator, z1, z2, n_steps=7):\n",
    "    \"\"\"Interpolate between two latent vectors\"\"\"\n",
    "    alphas = np.linspace(0, 1, n_steps + 2)  # 9 points total\n",
    "    interpolated = []\n",
    "    \n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        for alpha in alphas:\n",
    "            z = (1 - alpha) * z1 + alpha * z2\n",
    "            img = generator(z.unsqueeze(0))\n",
    "            interpolated.append(img)\n",
    "    \n",
    "    return torch.cat(interpolated, dim=0)\n",
    "\n",
    "# Generate two different random latent vectors\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "z1 = torch.randn(latent_dim, device=device)\n",
    "z2 = torch.randn(latent_dim, device=device)\n",
    "\n",
    "# Interpolate\n",
    "interpolated_images = interpolate_latent(generator, z1, z2, n_steps=7)\n",
    "\n",
    "# Display interpolation\n",
    "interpolated_images = (interpolated_images.cpu() + 1) / 2  # Denormalize\n",
    "grid = make_grid(interpolated_images, nrow=9, padding=2)\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.imshow(grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "plt.title('Latent Space Interpolation (9 images: start → 7 steps → end)')\n",
    "plt.axis('off')\n",
    "plt.savefig('latent_interpolation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GAN training curves\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(g_losses, label='Generator')\n",
    "plt.plot(d_losses, label='Discriminator')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('GAN Training Loss')\n",
    "plt.legend()\n",
    "plt.savefig('gan_training_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Trained Models\n",
    "\n",
    "Save the trained generator for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generator\n",
    "torch.save(generator.state_dict(), 'mnist_generator.pt')\n",
    "print('Generator saved to mnist_generator.pt')\n",
    "\n",
    "# To load later:\n",
    "# generator = Generator(latent_dim)\n",
    "# generator.load_state_dict(torch.load('mnist_generator.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Part I: LSTM\n",
    "- Implemented LSTM from scratch without using `torch.nn.LSTM`\n",
    "- Trained on palindrome sequences using RMSProp optimizer\n",
    "- Achieved near-perfect accuracy on T=5 sequences\n",
    "\n",
    "### Part II: GAN\n",
    "- Built Generator and Discriminator networks\n",
    "- Trained on MNIST dataset\n",
    "- Generated 25 sample images at different training stages\n",
    "- Demonstrated latent space interpolation between two digits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
